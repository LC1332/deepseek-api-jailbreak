## Prompt

```
对于DeepSeek新发布的模型，我想测试几家不同提供这个api的infra厂商的效果。

我想建立一个测试集合，这个测试集合可以自动化地进行测试，并给出准确率估计

请帮助我阅读，分析，总结DeepSeek论文。帮我给出合理的选取测试集合的策略

- DeepSeek论文中，进行了哪些测试。
- 这些测试集的规模如何，测试集关联的测试，是否可以自动完成（而不是像摘要任务那样需要人工干预）
- DeepSeekR1模型，比其他模型准确率高出了多少
```

## 数据集选取

MMLU-Pro上，DeepSeek-R1的准确率为84.0%，DeepSeek-V3为75.9%

LiveCodeBench上，DeepSeek-R1的Pass@1准确率为65.9%，DeepSeek-V3为36.2%，

在Codeforces上，DeepSeek-R1的Pass@1准确率为96.3%，DeepSeek-V3为58.7%

在AIME 2024上，DeepSeek-R1的Pass@1准确率为79.8%，DeepSeek-V3为39.2%

这四个里面我们挑两个做就可以了

## MMLU-Pro

https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro

但是这个一定要shots

这个其实是比较容易修改的

我们可以把 https://github.com/TIGER-AI-Lab/MMLU-Pro/blob/main/evaluate_from_api.py 代码中

line 148那里 load_dataset("TIGER-Lab/MMLU-Pro")

改为我们自己的subset（自己的subset从MMLU-Pro中抽样就可以）

## AIME 2024

https://huggingface.co/datasets/HuggingFaceH4/aime_2024

AIME就30道题，抽样一下，不测白不测。

## Code验证

这个可以稍后 因为code验证基本需要虚拟环境之类的，感觉有点小麻烦。


## 关联的概率问题

从 https://github.com/TIGER-AI-Lab/MMLU-Pro/tree/main/eval_results 中

我们实际上能够得到各个模型做各个题目的答案，这样我们其实可以主动挑选 K 个题目

来进行准确率测试。即讨论一个这样的问题

```
我们对13个模型，测试了12000个题目，每道题只有做错和做对两种可能。得到了13个模型在这个数据集上的准确率。

对于一个新模型，我们能否只测试一个很小的子集（比如10道题），就对它的准确率给出一个合理的估计？
```

Cheng: 这里wzq可以帮我解析eval_results中的数据，收集 模型和题目构成的矩阵，我来设计一套抽样和估计的算法。

我们对13个模型，测试了12000个题目，每道题只有做错和做对两种可能。得到了13个模型在这个数据集上的准确率。

对于一个新模型，我们能否只测试一个很小的子集（比如10道题），就对它的准确率给出一个合理的估计？


```
为了通过少量题目估计新模型的准确率，可采用以下步骤：

1. **题目选择策略**：
   - **分层抽样**：根据现有13个模型在每道题的平均正确率（难度）将题目分为难、中、易三层。每层按比例抽取题目（例如3:4:3），确保样本覆盖不同难度。
   - **高相关性选择**：计算每个题目与模型总准确率的点二列相关系数，选取相关系数最高的10题，这些题目能更好预测整体表现。

2. **估计方法**：
   - **直接估计**：测试新模型在选定10题上的正确率（k/10），作为总体准确率的点估计。计算置信区间（如Wilson score区间）以评估不确定性。
   - **回归模型**：利用现有13个模型的数据，训练线性回归模型，将10题的正确率作为特征预测总准确率。使用正则化（如岭回归）防止过拟合。
   - **贝叶斯估计**：结合现有模型的平均准确率作为先验（Beta分布），更新后验分布（Beta(α+k, β+10−k)），后验均值作为估计值。

3. **验证与优化**：
   - 通过交叉验证评估方法效果，例如留一法，计算平均绝对误差，调整题目选择策略或模型参数。

**结论**：通过合理选择题目子集（如分层或高相关题目）并结合统计方法（如回归或贝叶斯估计），仅测试10题即可对新模型准确率给出合理估计，尽管估计存在一定误差，但方法在效率和准确性间取得平衡。
```




